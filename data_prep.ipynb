{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Necessary Packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk \n",
    "import os \n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.linalg import norm\n",
    "from scipy.linalg import eigh\n",
    "from scipy.spatial.distance import pdist\n",
    "import scipy.cluster.hierarchy as sch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up import \n",
    "OHCO = ['book_id', 'chap_num', 'para_num', 'sent_num', 'token_num']\n",
    "path = '/Users/kritchanwong/Downloads/Plato-project-5001-main/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating LIB Tables\n",
    "title=['Timaeus', 'Theaetetus', 'Republic', 'Symposium',\n",
    "           'Statesman', 'Sophist', 'Protagoras', 'Philebus',\n",
    "           'Phaedrus', 'Phaedo', 'Parmenides', 'Meno',\n",
    "           'Menexenus', 'Lysis', 'Laws', 'Laches', 'Ion',\n",
    "           'Gorgias', 'Euthyphro', 'Euthydemus', 'Crito',\n",
    "           'Critias', 'Cratylus', 'Charmides', 'Apology']\n",
    "\n",
    "period= ['Late','Middle','Middle','Middle',\n",
    "         'Late','Late','Early','Late',\n",
    "         'Middle','Middle','Middle','Middle',\n",
    "         'Unknown','Early','Late','Early','Early',\n",
    "         'Early','Early','Middle','Early',\n",
    "         'Late','Middle','Early','Early']\n",
    "lib = {'book_id': title, 'Period': period,'Author': ['Plato']*25}\n",
    "LIB = pd.DataFrame(lib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make tokeniser function\n",
    "def tokeniser(epub_file, first_line, book_val, chap_val):\n",
    "    # Reading in the Epub\n",
    "    epub = open(path + '/data_in/' + epub_file, encoding= \"utf8\", \\\n",
    "    errors= 'ignore').readlines()\n",
    "    df = pd.DataFrame(epub, columns=['line_str'])\n",
    "    df.index.name = 'line_num'\n",
    "    df.line_str = df.line_str.str.strip()\n",
    "        \n",
    "    # Removing Cruft\n",
    "    a = df.line_str.str.match(first_line)\n",
    "    b = df.tail(1)\n",
    "    an = df.loc[a].index[0]\n",
    "    bn = b.index[0]\n",
    "    df = df.loc[an: bn] \n",
    "    \n",
    "    # Add Book ID\n",
    "    df['book_id'] = epub_file.split('.')[0] #Adding Book ID\n",
    "\n",
    "    # Dealing with Chapters\n",
    "    if book_val == 0:\n",
    "        chap_lines = df.line_str.str.match(first_line)\n",
    "    else: \n",
    "        chap_lines = df.line_str.str.match(chap_val)\n",
    "    chap_nums = [i+1 for i in range(df.loc[chap_lines].shape[0])]\n",
    "    df.loc[chap_lines, 'chap_num'] = chap_nums\n",
    "    df.chap_num = df.chap_num.ffill() #Fill in the NANs with chapter number\n",
    "    df = df.loc[~chap_lines] # Remove chapter heading lines\n",
    "    df.chap_num = df.chap_num.astype('int') # Convert chap_num from float to int\n",
    "    \n",
    "    # Creating dataframe based on CHAPTER\n",
    "    dfc = df.groupby(OHCO[:2]).line_str.apply(lambda x: '\\n'.join(x)).to_frame() # Make big string\n",
    "    dfc['line_str'] = dfc.line_str.str.strip()\n",
    "\n",
    "    # Creating dataframe based on PARAGRAPH\n",
    "    # Creating dataframe based on PARAGRAPHS\n",
    "    dfp = dfc['line_str'].str.split(r'\\n\\n+', expand=True).stack()\\\n",
    "        .to_frame().rename(columns={0:'para_str'}) #Grouping by Paragraphs\n",
    "    dfp.index.names = OHCO[:3]\n",
    "    dfp = dfp[~dfp['para_str'].str.match(r'^\\s*$')] # Remove empty paragraphs\n",
    "    \n",
    "    # Creating dataframes based on SENTENCE and TOKENS using NLTK\n",
    "    # Paragraphs to Sentences\n",
    "    dfs= dfp.para_str\\\n",
    "        .apply(lambda x: pd.Series(nltk.sent_tokenize(x)))\\\n",
    "        .stack()\\\n",
    "        .to_frame()\\\n",
    "        .rename(columns={0:'sent_str'})\n",
    "\n",
    "    # Sentences to Tokens\n",
    "    # Local function to pick tokenizer\n",
    "    def word_tokenize(x):\n",
    "        s = pd.Series(nltk.pos_tag(nltk.word_tokenize(x)))\n",
    "        return s\n",
    "    \n",
    "    # Tokenised Dataframe            \n",
    "    dft = dfs.sent_str\\\n",
    "        .apply(word_tokenize)\\\n",
    "        .stack()\\\n",
    "        .to_frame()\\\n",
    "        .rename(columns={0:'pos_tuple'})\n",
    "        \n",
    "    # Grab info from tuple\n",
    "    dft['pos'] = dft.pos_tuple.apply(lambda x: x[1])\n",
    "    dft['token_str'] = dft.pos_tuple.apply(lambda x: x[0])\n",
    "    return dft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing Data\n",
    "Apology_token = tokeniser('Apology.txt',r'APOLOGY.',0,0)\n",
    "Republic_token = tokeniser('The Republic.txt',r'\\s*BOOK I',1,\n",
    "                         r\"^\\s*(BOOK|letter)\\s+(d+|I|II|III|IV|V|VI|VII|VIII|IX|X|XX)\")\n",
    "Laws_token = tokeniser('Laws.txt',r'\\s*BOOK I\\.',1,r\"^\\s*(BOOK|letter)\\s+(|I|II|III|IV|V|VI|VII|VIII|IIX|IX|X|XI|XII)\\.\")\n",
    "Charmides_token = tokeniser('Charmides.txt',r'\\s*PERSONS\\s*\\s*OF\\s*\\s*THE\\s*DIALOGUE.*',0,0)\n",
    "Cratylus_token = tokeniser('Cratylus.txt',r'\\s*PERSONS\\s*\\s*OF\\s*\\s*THE\\s*DIALOGUE.*',0,0)\n",
    "Crito_token = tokeniser('Crito.txt',r'\\s*PERSONS\\s*\\s*OF\\s*\\s*THE\\s*DIALOGUE.*',0,0)\n",
    "Critias_token = tokeniser('Critias.txt',r'\\s*PERSONS\\s*\\s*OF\\s*\\s*THE\\s*DIALOGUE.*',0,0)\n",
    "Euthydemus_token = tokeniser('Euthydemus.txt',r'\\s*PERSONS\\s*\\s*OF\\s*\\s*THE\\s*DIALOGUE.*',0,0)\n",
    "Euthyphro_token = tokeniser('Euthyphro.txt',r'\\s*PERSONS\\s*\\s*OF\\s*\\s*THE\\s*DIALOGUE.*',0,0)\n",
    "Gorgias_token = tokeniser('Gorgias.txt',r'\\s*PERSONS\\s*\\s*OF\\s*\\s*THE\\s*DIALOGUE.*',0,0)\n",
    "Ion_token = tokeniser('Ion.txt',r'\\s*PERSONS\\s*\\s*OF\\s*\\s*THE\\s*DIALOGUE.*',0,0)\n",
    "Laches_token = tokeniser('Laches.txt',r'\\s*PERSONS\\s*\\s*OF\\s*\\s*THE\\s*DIALOGUE.*',0,0)\n",
    "Lysis_token = tokeniser('Lysis.txt',r'\\s*PERSONS\\s*\\s*OF\\s*\\s*THE\\s*DIALOGUE.*',0,0)\n",
    "Menexenus_token = tokeniser('Menexenus.txt',r'\\s*PERSONS\\s*\\s*OF\\s*\\s*THE\\s*DIALOGUE.*',0,0)\n",
    "Meno_token = tokeniser('Menexenus.txt',r'\\s*PERSONS\\s*\\s*OF\\s*\\s*THE\\s*DIALOGUE.*',0,0)\n",
    "Parmenides_token = tokeniser('Parmenides.txt',r'\\s*PERSONS\\s*\\s*OF\\s*\\s*THE\\s*DIALOGUE.*',0,0)\n",
    "Phaedo_token = tokeniser('Phaedo.txt',r'\\s*PERSONS\\s*\\s*OF\\s*\\s*THE\\s*DIALOGUE.*',0,0)\n",
    "Phaedrus_token = tokeniser('Phaedrus.txt',r'\\s*PERSONS\\s*\\s*OF\\s*\\s*THE\\s*DIALOGUE.*',0,0)\n",
    "Philebus_token = tokeniser('Philebus.txt',r'\\s*PERSONS\\s*\\s*OF\\s*\\s*THE\\s*DIALOGUE.*',0,0)\n",
    "Protagoras_token = tokeniser('Protagoras.txt',r'\\s*PERSONS\\s*\\s*OF\\s*\\s*THE\\s*DIALOGUE.*',0,0)\n",
    "Sophist_token = tokeniser('Sophist.txt',r'\\s*PERSONS\\s*\\s*OF\\s*\\s*THE\\s*DIALOGUE.*',0,0)\n",
    "Statesman_token = tokeniser('Statesman.txt',r'\\s*PERSONS\\s*\\s*OF\\s*\\s*THE\\s*DIALOGUE.*',0,0)\n",
    "Symposium_token = tokeniser('Symposium.txt',r'\\s*PERSONS\\s*\\s*OF\\s*\\s*THE\\s*DIALOGUE.*',0,0)\n",
    "Theaetetus_token = tokeniser('Theaetetus.txt', r'\\s*PERSONS\\s*\\s*OF\\s*\\s*THE\\s*DIALOGUE.*',0,0)\n",
    "Timaeus_token = tokeniser('Timaeus.txt', r'\\s*PERSONS\\s*\\s*OF\\s*\\s*THE\\s*DIALOGUE.*',0,0)\n",
    "### Plato Tokens\n",
    "TOKEN = pd.concat([Timaeus_token, Theaetetus_token, Republic_token, Symposium_token,\n",
    "           Statesman_token, Sophist_token, Protagoras_token, Philebus_token,\n",
    "           Phaedrus_token, Phaedo_token, Parmenides_token, Meno_token,\n",
    "           Menexenus_token, Lysis_token, Laws_token, Laches_token, Ion_token,\n",
    "           Gorgias_token, Euthyphro_token, Euthydemus_token, Crito_token,\n",
    "           Critias_token, Cratylus_token, Charmides_token, Apology_token])\n",
    "TOKEN = TOKEN.rename_axis(['book_id','chap_num','para_num','sent_num','token_num'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create BOW and VOCAB tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3f/42p6mrgd1jj65bcnbf2ztbkc0000gn/T/ipykernel_32184/2717997460.py:2: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  TOKEN['term_str']=TOKEN['token_str'].str.lower().str.replace('[\\W_]', '')\n"
     ]
    }
   ],
   "source": [
    "# Annotated Vocab Table\n",
    "TOKEN['term_str']=TOKEN['token_str'].str.lower().str.replace('[\\W_]', '')\n",
    "VOCAB = TOKEN.term_str.value_counts().to_frame().rename(columns={'index':'term_str', 'term_str':'n'})\\\n",
    "    .sort_index().reset_index().rename(columns={'index':'term_str'})\n",
    "VOCAB.index.name = 'term_id'\n",
    "VOCAB['num'] = VOCAB.term_str.str.match(\"\\d+\").astype('int')\n",
    "sw = pd.DataFrame(nltk.corpus.stopwords.words('english'), columns=['term_str'])\n",
    "sw = sw.reset_index().set_index('term_str')\n",
    "sw.columns = ['dummy']\n",
    "sw.dummy = 1\n",
    "VOCAB['stop'] = VOCAB.term_str.map(sw.dummy)\n",
    "VOCAB['stop'] = VOCAB['stop'].fillna(0).astype('int')\n",
    "stemmer = PorterStemmer()\n",
    "VOCAB['p_stem'] = VOCAB.term_str.apply(stemmer.stem)\n",
    "TOKEN['pos_group'] = TOKEN.pos.str[:2]\n",
    "pos_max = TOKEN.groupby(['term_str','pos_group']).pos.count().unstack().idxmax(1)\n",
    "VOCAB['pos_max'] = list(pos_max)\n",
    "VOCAB['term_code'] = VOCAB.term_str + '/' + VOCAB.pos_max\n",
    "N_vocab = VOCAB.shape[0]\n",
    "U_vocab = 1/N_vocab\n",
    "VOCAB['p'] = VOCAB.n / VOCAB.n.sum()  # Probability using MLE\n",
    "VOCAB['s'] = 1 / VOCAB.p              # Surprise\n",
    "VOCAB['i'] = np.log2(VOCAB.s)         # Information\n",
    "VOCAB['h'] = VOCAB.p * VOCAB.i        # Entropy\n",
    "VOCAB['wlen'] = VOCAB.term_str.str.len() # Word length feature\n",
    "VOCAB = VOCAB.sort_values('n', ascending=False)\n",
    "VOCAB['term_rank'] = [r+1 for r in range(VOCAB.shape[0])] ### Compute the term rank, and sort table\n",
    "## BOW\n",
    "DOC = OHCO[:1]\n",
    "BOW = TOKEN.groupby(DOC+['term_str']).term_str.count().to_frame('tf_n')\n",
    "D = BOW.groupby(DOC).tf_n # many ways to calculate term frequency\n",
    "BOW['tf_jp'] = D.apply(lambda x: x / x.sum().sum()) # jp = P(w,d)\n",
    "BOW['tf_cp'] = D.apply(lambda x: x / x.sum()) # cp = P(w|d)\n",
    "BOW['tf_l2'] = D.apply(lambda x: x / np.sqrt((x**2).sum()))\n",
    "BOW['tf_logn'] = D.apply(lambda x: np.log2(1 + x))\n",
    "BOW['tf_sub'] = D.apply(lambda x: 1 + np.log2(x)) # Sublinear scaling; from Manning, et al.\n",
    "BOW['tf_max'] = D.apply(lambda x: .4 + .6 * (x / x.max())) # See Manning, et al. for choice of α\n",
    "BOW['tf_bool'] = D.apply(lambda x: x.astype('bool') / x.astype('bool').sum())\n",
    "VOCAB['df']=list(BOW.groupby('term_str').tf_n.count())\n",
    "N_docs = len(D.groups)\n",
    "VOCAB['idf'] = list(np.log2(N_docs/VOCAB.df)) \n",
    "tf_types = [col.split('_')[1] for col in BOW.columns.to_list() if 'tf_' in col]\n",
    "VOCAB=VOCAB.set_index('term_str')\n",
    "for tf_type in tf_types:\n",
    "    BOW[f'tfidf_{tf_type}'] = BOW[f'tf_{tf_type}'] * VOCAB.idf\n",
    "for tf_type in tf_types:\n",
    "    col = f\"tfidf_{tf_type}\"\n",
    "    VOCAB[col + \"_sum\"] = BOW.groupby('term_str')[col].sum()\n",
    "    VOCAB[col + \"_sum\"] = (VOCAB[col + \"_sum\"] - VOCAB[col + \"_sum\"].mean()) / VOCAB[col + \"_sum\"].std()\n",
    "    VOCAB[col + \"_sum\"] = VOCAB[col + \"_sum\"] - VOCAB[col + \"_sum\"].min() \n",
    "    VOCAB[col + \"_sum\"] = VOCAB[col + \"_sum\"] / N_docs\n",
    "tfidf_sum_cols = [f\"tfidf_{type}_sum\" for type in tf_types]\n",
    "## Making TFIDF \n",
    "tfidf_sum = 'tfidf_bool'\n",
    "TFIDF = BOW.groupby(OHCO[:1]+['term_str'])[[tfidf_sum]].mean().unstack(fill_value=0)\n",
    "TFIDF.columns = TFIDF.columns.droplevel(0) #dropping all 0s\n",
    "VOCAB['dfidf'] = VOCAB['idf']*VOCAB['df'] \n",
    "VOCAB_sig = VOCAB.sort_values(by='dfidf',ascending=False).head(4000)\n",
    "TFIDF = TFIDF[VOCAB_sig.index] #choosing 4000 top terms\n",
    "TFIDF = TFIDF.apply(lambda x: x / np.sqrt(np.square(x).sum()), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIB.to_csv(path + '/data_out/' + 'lib_plato.csv')\n",
    "BOW.to_csv(path + '/data_out/' + 'bow_plato.csv')\n",
    "VOCAB.to_csv(path + '/data_out/' +'vocab_plato.csv')\n",
    "TOKEN.to_csv(path + '/data_out/' +'token_plato.csv' )\n",
    "TFIDF.to_csv(path + '/data_out/' +'tf_idf_plato.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
